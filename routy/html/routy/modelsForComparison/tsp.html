<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>routy.modelsForComparison.tsp API documentation</title>
<meta name="description" content="anaparastash-thesis-tsp.ipynb …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>routy.modelsForComparison.tsp</code></h1>
</header>
<section id="section-intro">
<p>anaparastash-thesis-tsp.ipynb</p>
<p>Automatically generated by Colaboratory.</p>
<p>Original file is located at
<a href="https://colab.research.google.com/drive/15yQTIBTTF6KmQAKvciV2OSSLO_pl-EMy">https://colab.research.google.com/drive/15yQTIBTTF6KmQAKvciV2OSSLO_pl-EMy</a></p>
<h2>Επίλυση προβλήματος περιοδεύοντος πωλητή με ενισχυτική μάθηση</h2>
<p>Στο παρόν σημειωματάριο θα λύσουμε το πρόβλημα του περιοδεύοντος πωλητή με ενισχυτική μάθηση. </p>
<p>Εκπαιδεύουμε ένα νευρωνικό δίκτυο το οποίο λαμβάνει τις συντεταγμένες των πόλεων που θέλουμε να επισκεφτούμε και επιστρέφει μια κατανομή πιθανοτήτων επίσκεψης της κάθε πόλης κάθε χρονική στιγμή. Το τελικό αποτέλεσμα είναι η εύρεση της σειράς με την οποία θα πρέπει να επισκεφτούμε τις πόλεις με τον πιο αποδοτικό τρόπο. </p>
<p>Η επιβράβευση του πράκτορα ενισχυτικής μάθησης είναι η αρνητική απόσταση που διανύει το φορτηγό για να επισκεφτεί όλες τις πόλεις με τη σειρά που προτείνει το μοντέλο. Το μοντέλο χρησιμοποιεί έναν αλγόριθμο κλίσης (policy gradient) για να βελτιστοποιήσει την πολιτική του πράκτορα (policy) για να μεγιστοποιήσει την επιβράβευση.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;anaparastash-thesis-tsp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15yQTIBTTF6KmQAKvciV2OSSLO_pl-EMy

&lt;h2&gt;Επίλυση προβλήματος περιοδεύοντος πωλητή με ενισχυτική μάθηση&lt;/h2&gt;

&lt;p&gt;Στο παρόν σημειωματάριο θα λύσουμε το πρόβλημα του περιοδεύοντος πωλητή με ενισχυτική μάθηση. &lt;/p&gt;
&lt;p&gt;Εκπαιδεύουμε ένα νευρωνικό δίκτυο το οποίο λαμβάνει τις συντεταγμένες των πόλεων που θέλουμε να επισκεφτούμε και επιστρέφει μια κατανομή πιθανοτήτων επίσκεψης της κάθε πόλης κάθε χρονική στιγμή. Το τελικό αποτέλεσμα είναι η εύρεση της σειράς με την οποία θα πρέπει να επισκεφτούμε τις πόλεις με τον πιο αποδοτικό τρόπο. &lt;/p&gt;
&lt;p&gt;Η επιβράβευση του πράκτορα ενισχυτικής μάθησης είναι η αρνητική απόσταση που διανύει το φορτηγό για να επισκεφτεί όλες τις πόλεις με τη σειρά που προτείνει το μοντέλο. Το μοντέλο χρησιμοποιεί έναν αλγόριθμο κλίσης (policy gradient) για να βελτιστοποιήσει την πολιτική του πράκτορα (policy) για να μεγιστοποιήσει την επιβράβευση.&lt;/p&gt;
&#34;&#34;&#34;

# Commented out IPython magic to ensure Python compatibility.
import math
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
import torch.nn.functional as F
from torch.autograd import Variable
from torch.utils.data import Dataset, DataLoader
import time
from IPython.display import clear_output
from tqdm import tqdm
import matplotlib

matplotlib.use(&#39;Agg&#39;)
import matplotlib.pyplot as plt

# %matplotlib inline

&#34;&#34;&#34;Δημιουργούμε τα δεδομένα του προβλήματος που είναι οι συντεταγμένες των πόλεων.&#34;&#34;&#34;


class TSPDataset(Dataset):
    def __init__(self, num_nodes, num_samples, random_seed=111):
        super(TSPDataset, self).__init__()
        torch.manual_seed(random_seed)

        self.data_set = []
        for l in tqdm(range(num_samples)):
            x = torch.FloatTensor(2, num_nodes).uniform_(0, 1)
            self.data_set.append(x)

        self.size = len(self.data_set)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data_set[idx]


class TSPDatasetGaussian(Dataset):
    def __init__(self, num_nodes, num_samples, random_seed=111):
        super(TSPDatasetGaussian, self).__init__()
        torch.manual_seed(random_seed)

        self.data_set = []
        for l in tqdm(range(num_samples)):
            m = torch.distributions.normal.Normal(torch.tensor(0.0), torch.tensor(1.0))
            x = m.sample(sample_shape=(2, num_nodes))
            self.data_set.append(x)

        self.size = len(self.data_set)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data_set[idx]


&#34;&#34;&#34;Δημιουργούμε τις κλάσεις με τα δεδομένα του προβλήματος&#34;&#34;&#34;

train_size = 100  # _000
val_size = 1000

train_20_dataset = TSPDataset(20, train_size)
val_20_dataset = TSPDataset(20, val_size)
train_20_datasetGauss = TSPDatasetGaussian(20, train_size)
val_20_datasetGauss = TSPDatasetGaussian(20, val_size)

train_50_dataset = TSPDataset(50, train_size)
val_50_dataset = TSPDataset(50, val_size)

train_100_dataset = TSPDataset(100, train_size)
val_100_dataset = TSPDataset(100, val_size)

&#34;&#34;&#34;&lt;h3&gt;Ανταμοιβή&lt;/h3&gt;
&lt;p&gt;
Δίνοντας ως είσοδο τις συντεταγμένες των πόλεων, έστω ν ο αριθμός των πόλεων, ο στόχος είναι η εύρεση της σειράς π των πόλεων που πρέπει να επισκεφτούμε ώστε το συνολικό μήκος της διαδρομής που διανύσαμε να είναι ελάχιστο. Ορίζουμε την συνάρτηση που υπολογίζει το μήκος των διαδρομών.
&lt;/p&gt;
&#34;&#34;&#34;


def reward(sample_solution):
    &#34;&#34;&#34;
    sample_solution seq_len of [batch_size]
    &#34;&#34;&#34;
    batch_size = sample_solution[0].size(0)
    n = len(sample_solution)
    tour_len = Variable(torch.zeros([batch_size]))

    for i in range(n - 1):
        tour_len += torch.norm(sample_solution[i] - sample_solution[i + 1], dim=1)

    tour_len += torch.norm(sample_solution[n - 1] - sample_solution[0], dim=1)

    return tour_len


&#34;&#34;&#34;&lt;h3&gt;Μηχανισμοί προσοχής&lt;/h3&gt;
&lt;p&gt;
Χρησιμοποιούμε δυο μηχανισμούς προσοχής που λέγονται γινόμενο (&#34;Dot&#34;) και &#34;Bahdanau&#34;.&lt;/a&gt;&lt;/p&gt;

$$
a_t(s) = align(h_t, \bar h_s)  = \dfrac{exp(score(h_t, \bar h_s))}{\sum_{s&#39;} exp(score(h_t, \bar h_{s&#39;}))}
$$

$$
score(h_t, \bar h_s) =
\begin{cases}
h_t ^\top \bar h_s &amp; Dot \\
v_a ^\top \tanh(\textbf{W}_a [ h_t ; \bar h_s ]) &amp; Bahdanau
\end{cases}
$$
&#34;&#34;&#34;


class Attention(nn.Module):
    def __init__(self, hidden_size, use_tanh=False, C=10, name=&#39;Bahdanau&#39;):
        super(Attention, self).__init__()

        self.use_tanh = use_tanh
        self.C = C
        self.name = name

        if name == &#39;Bahdanau&#39;:
            self.W_query = nn.Linear(hidden_size, hidden_size)
            self.W_ref = nn.Conv1d(hidden_size, hidden_size, 1, 1)

            V = torch.FloatTensor(hidden_size)
            self.V = nn.Parameter(V)
            self.V.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))

    def forward(self, query, ref):
        &#34;&#34;&#34;
        Args:
            query: [batch_size x hidden_size]
            ref:   ]batch_size x seq_len x hidden_size]
        &#34;&#34;&#34;

        batch_size = ref.size(0)
        seq_len = ref.size(1)

        if self.name == &#39;Bahdanau&#39;:
            ref = ref.permute(0, 2, 1)
            query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]
            ref = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]
            expanded_query = query.repeat(1, 1, seq_len)  # [batch_size x hidden_size x seq_len]
            V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size x 1 x hidden_size]
            logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)

        elif self.name == &#39;Dot&#39;:
            query = query.unsqueeze(2)
            logits = torch.bmm(ref, query).squeeze(2)  # [batch_size x seq_len x 1]
            ref = ref.permute(0, 2, 1)

        else:
            raise NotImplementedError

        if self.use_tanh:
            logits = self.C * F.tanh(logits)
        else:
            logits = logits
        return ref, logits


&#34;&#34;&#34;&lt;h3&gt;Ενσωμάτωση γράφου&lt;/h3&gt;
&lt;p&gt; Χρησιμοποιούμε μια ενσωμάτωση γράφου για τα χαρακτηριστικά που θα λάβει το μοντέλο. Για πιο εξελιγμένες μεθόδους ενσωμάτωσης υπάρχει αυτό το άρθρο &lt;a href=&#34;https://arxiv.org/pdf/1705.02801.pdf&#34;&gt;Graph Embedding Techniques, Applications, and Performance: A Survey&lt;/a&gt;
&lt;/p&gt;
&#34;&#34;&#34;


class GraphEmbedding(nn.Module):
    def __init__(self, input_size, embedding_size):
        super(GraphEmbedding, self).__init__()
        self.embedding_size = embedding_size

        self.embedding = nn.Parameter(torch.FloatTensor(input_size, embedding_size))
        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))

    def forward(self, inputs):
        batch_size = inputs.size(0)
        seq_len = inputs.size(2)
        embedding = self.embedding.repeat(batch_size, 1, 1)
        embedded = []
        inputs = inputs.unsqueeze(1)
        for i in range(seq_len):
            embedded.append(torch.bmm(inputs[:, :, :, i].float(), embedding))
        embedded = torch.cat(embedded, 1)
        return embedded


class ConvolutionalGraphEmbedding(nn.Module):
    def __init__(self, input_size, embedding_size):
        super(ConvolutionalGraphEmbedding, self).__init__()
        self.embedding_size = embedding_size
        self.embedding = nn.Conv1d(input_size, embedding_size, kernel_size=1)

    def forward(self, inputs):
        batch_size = inputs.size(0)
        embedded = []
        inputs = inputs.unsqueeze(1)
        for i in range(batch_size):
            el = self.embedding(inputs[i, 0])
            embedded.append(torch.unsqueeze(el, 0))
        embedded = torch.cat(embedded, 0)
        return embedded.transpose(2, 1)


&#34;&#34;&#34;&lt;h3&gt;Δίκτυα δείκτη (&lt;a href=&#34;https://arxiv.org/abs/1506.03134&#34;&gt;Pointer Networks
&lt;/a&gt;)&lt;/h3&gt;
Το μοντέλο αυτό λύνει το πρόβλημα εύρεσης ακολουθίας εξόδου μεταβλητού μεγέθους χρησιμοποιώντας εναν μηχανισμό προσοχής. Ο μηχανισμός αυτός προσοχής, αντί να χρησιμοποιεί την προσοχή για να &#34;αναμείξει&#34; τις κρυμμένες καταστάσεις του κωδικοποιητή σε ένα διάνυσμα πλαισίου (context vector) σε κάθε βήμα αποκωδικοποίησης, χρησιμοποιεί την προσοχή για να &#34;δείξει&#34; σε ένα μέλος της ακολουθίας εισόδου και να το δώσει ως αποτέλεσμα. Αυτό έχει ως συνέπεια να μην χρειάζεται να ξέρουμε εκ των προτέρων το μήκος της ακολουθίας εξόδου. Συνεπώς το ίδιο μοντέλο μπορεί να εκπαιδευτεί για διαφορετικά μήκοι ακολουθίας πόλεων (πχ για 20 ή 50 πόλεις).

&#34;&#34;&#34;


class PointerNet(nn.Module):
    def __init__(self,
                 embedding_size,
                 hidden_size,
                 seq_len,
                 n_glimpses,
                 tanh_exploration,
                 use_tanh,
                 attention,
                 embedding=&#34;Graph&#34;):
        super(PointerNet, self).__init__()

        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.n_glimpses = n_glimpses
        self.seq_len = seq_len

        if embedding == &#34;Graph&#34;:
            self.embedding = GraphEmbedding(2, embedding_size)
        elif embedding == &#34;Conv&#34;:
            self.embedding = ConvolutionalGraphEmbedding(2, embedding_size)

        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)
        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)
        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, name=attention)
        self.glimpse = Attention(hidden_size, use_tanh=False, name=attention)

        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))
        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))

    def apply_mask_to_logits(self, logits, mask, idxs):
        batch_size = logits.size(0)
        clone_mask = mask.clone()

        if idxs is not None:
            clone_mask[[i for i in range(batch_size)], idxs.data] = 1
            logits[clone_mask] = -np.inf

        return logits, clone_mask

    def forward(self, inputs):
        &#34;&#34;&#34;
        Args:
            inputs: [batch_size x 1 x sourceL]
        &#34;&#34;&#34;
        batch_size = inputs.size(0)
        seq_len = inputs.size(2)

        embedded = self.embedding(inputs)
        encoder_outputs, (hidden, context) = self.encoder(embedded)

        prev_probs = []
        prev_idxs = []
        mask = torch.zeros(batch_size, seq_len).byte()

        idxs = None

        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)

        for i in range(seq_len):

            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))

            query = hidden.squeeze(0)
            for i in range(self.n_glimpses):
                ref, logits = self.glimpse(query, encoder_outputs)
                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)

            _, logits = self.pointer(query, encoder_outputs)
            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
            probs = F.softmax(logits)

            idxs = probs.multinomial(1).squeeze(1)

            for old_idxs in prev_idxs:
                if old_idxs.eq(idxs).data.any():
                    print(seq_len)
                    print(&#39; RESAMPLE!&#39;)
                    idxs = probs.multinomial(1).squeeze(1)
                    break

            decoder_input = embedded[[i for i in range(batch_size)], idxs.data, :]

            prev_probs.append(probs)
            prev_idxs.append(idxs)

        return prev_probs, prev_idxs


&#34;&#34;&#34;&lt;h3&gt;Βελτιστοποίηση πολιτικής με τον αλγόριθμο κλίσης πολιτικής&lt;/h3&gt;
&lt;p&gt;Χρησιμοποιούμε ενισχυτική μάθηση με πολιτική (model-free policy-based Reinforcement Learning) με σκοπό την εκπαίδευση ενός δικτύου δείκτη με τον αλγόριθμο REINFORCE.&lt;/p&gt;
&#34;&#34;&#34;


class CombinatorialRL(nn.Module):
    def __init__(self,
                 embedding_size,
                 hidden_size,
                 seq_len,
                 n_glimpses,
                 tanh_exploration,
                 use_tanh,
                 reward,
                 attention,
                 embedding):
        super(CombinatorialRL, self).__init__()
        self.reward = reward

        self.actor = PointerNet(
            embedding_size,
            hidden_size,
            seq_len,
            n_glimpses,
            tanh_exploration,
            use_tanh,
            attention,
            embedding)

    def forward(self, inputs):
        &#34;&#34;&#34;
        Args:
            inputs: [batch_size, input_size, seq_len]
        &#34;&#34;&#34;
        batch_size = inputs.size(0)
        input_size = inputs.size(1)
        seq_len = inputs.size(2)

        probs, action_idxs = self.actor(inputs)

        actions = []
        inputs = inputs.transpose(1, 2)
        for action_id in action_idxs:
            actions.append(inputs[[x for x in range(batch_size)], action_id.data, :])

        action_probs = []
        for prob, action_id in zip(probs, action_idxs):
            action_probs.append(prob[[x for x in range(batch_size)], action_id.data])

        R = self.reward(actions)

        return R, action_probs, actions, action_idxs


&#34;&#34;&#34;Ορίζουμε τις υπερπαραμέτρους του δικτύου.&#34;&#34;&#34;

embedding_size = 128
hidden_size = 128
n_glimpses = 1
tanh_exploration = 10
use_tanh = True
batch_size = 128
beta = 0.9
max_grad_norm = 2.

&#34;&#34;&#34;Δημιουργούμε τα μοντέλα που θα εκπαιδεύσουμε.&#34;&#34;&#34;

tsp_20_model_dot_attention = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward,
    embedding=&#34;Graph&#34;,
    attention=&#34;Dot&#34;)

tsp_20_model_dot_attention_gaussian = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding=&#34;Graph&#34;,
    attention=&#34;Dot&#34;)

tsp_50_model_bahdanau_attention = CombinatorialRL(
    embedding_size,
    hidden_size,
    50,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding=&#34;Graph&#34;,
    attention=&#34;Bahdanau&#34;)

tsp_20_model_conv_embedding = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward,
    embedding=&#34;Conv&#34;,
    attention=&#34;Dot&#34;)

&#34;&#34;&#34;&lt;h3&gt;Κλάση για εκπαίδευση μοντέλου&lt;/h3&gt;

Στη συνέχεια ορίζουμε μια κλάση στην οποία εκπαιδεύεται το μοντέλο.
&#34;&#34;&#34;


class TrainModel:
    def __init__(self, model, train_dataset, val_dataset, batch_size=128, threshold=None, max_grad_norm=2.,
                 modelName=&#34;model.pt&#34;):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.batch_size = batch_size
        self.threshold = threshold
        self.modelName = modelName
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)

        self.actor_optim = optim.Adam(model.actor.parameters(), lr=1e-4)
        self.max_grad_norm = max_grad_norm

        self.train_tour = []
        self.val_tour = []
        self.advantage = []
        self.train_loss = []
        self.distances = []
        self.epochs = 0

    def train_and_validate(self, n_epochs):
        critic_exp_mvg_avg = torch.zeros(1)
        start_time = time.time()

        for epoch in range(n_epochs):
            loss_at_epoch = 0.0
            distance = 0.0
            val_distance = 0.0
            advantage_at_epoch = 0.0

            for batch_id, sample_batch in enumerate(self.train_loader):
                self.model.train()

                inputs = Variable(sample_batch)

                R, probs, actions, actions_idxs = self.model(inputs)

                if batch_id == 0:
                    critic_exp_mvg_avg = R.mean()
                else:
                    critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())

                advantage = R - critic_exp_mvg_avg
                advantage_at_epoch += advantage.detach().mean().item()

                logprobs = 0
                for prob in probs:
                    logprob = torch.log(prob)
                    logprobs += logprob
                logprobs[logprobs &lt; -1000] = 0.

                reinforce = advantage * logprobs
                actor_loss = reinforce.mean()

                self.actor_optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm(self.model.actor.parameters(),
                                              float(self.max_grad_norm), norm_type=2)

                self.actor_optim.step()

                critic_exp_mvg_avg = critic_exp_mvg_avg.detach()

                loss_at_epoch += actor_loss.detach().item()
                distance += R.mean().data.item()

                if batch_id % 100 == 0:
                    self.model.eval()
                    for val_batch in self.val_loader:
                        inputs = Variable(val_batch)

                        R, probs, actions, actions_idxs = self.model(inputs)
                        val_distance += R.mean().data.item()

            # epoch finished

            self.val_tour.append(val_distance)
            self.train_loss.append(loss_at_epoch)
            self.train_tour.append(distance)
            self.advantage.append(advantage_at_epoch)
            self.plotTrainAndValidationTourLength(self.epochs)
            self.plotTrainLossAndAdvantage(self.epochs)

            torch.save(self.model.state_dict(), self.modelName)
            if self.threshold and self.train_tour[-1] &lt; self.threshold:
                print(f&#34;Early stopping at epoch {epoch}&#34;)
                break

            self.epochs += 1

        # Training finished
        training_time_in_secs = time.time() - start_time
        print(f&#39;Trainig for {training_time_in_secs // 60} minutes&#39;)

    def plotTrainLossAndAdvantage(self, epoch):
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.ylabel(&#34;Training loss&#34;)
        plt.xlabel(&#34;Epoch&#34;)
        plt.title(&#39;Loss: epoch %s loss %s&#39; % (epoch, self.train_tour[-1] if len(self.train_tour) else &#39;collecting&#39;))
        plt.plot(self.train_loss)
        plt.grid()
        plt.subplot(132)
        plt.title(
            &#39;Advantage: epoch %s reward %s&#39; % (epoch, self.advantage[-1] if len(self.advantage) else &#39;collecting&#39;))
        plt.plot(self.advantage)
        plt.grid()
        plt.ylabel(&#34;Training advantage&#34;)
        plt.xlabel(&#34;Epoch&#34;)
        plt.savefig(f&#34;trainLossAndAdvantage{self.modelName}.png&#34;)

        plt.show()
        plt.clf()

    def plotTrainAndValidationTourLength(self, epoch):
        clear_output(True)
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.ylabel(&#34;Tour length&#34;)
        plt.xlabel(&#34;Epoch&#34;)
        plt.title(&#39;train tour length: epoch %s,tour length %s&#39; % (
            epoch, self.train_tour[-1] if len(self.train_tour) else &#39;collecting&#39;))
        plt.plot(self.train_tour)
        plt.grid()
        plt.subplot(132)
        plt.title(&#39;val tour length: epoch %s,tour length %s&#39; % (
        epoch, self.val_tour[-1] if len(self.val_tour) else &#39;collecting&#39;))
        plt.plot(self.val_tour)
        plt.grid()
        plt.savefig(f&#34;tourLength{self.modelName}.png&#34;)

        plt.show()
        plt.clf()


&#34;&#34;&#34;Πρόβλημα περιοδεύοντος πωλητή για 20 πόλεις με μηχανισμό προσοχής γινομένου.

Στη συνέχεια ορίζουμε τη συνάρτηση για την αναπαράσταση της διαδρομής που ακολουθεί ο πωλητής για διάφορες πόλεις
&#34;&#34;&#34;

test_size = 2
test_20_dataset = TSPDataset(20, test_size)
test_loader_20nodes = DataLoader(test_20_dataset, batch_size=100, shuffle=True, num_workers=1)

test_5_dataset = TSPDataset(5, test_size)
test_loader_5nodes = DataLoader(test_5_dataset, batch_size=100, shuffle=True, num_workers=1)

&#34;&#34;&#34;Στη συνέχεια φορτώνουμε τα μοντέλα που εκπαιδεύσαμε και βλέπουμε τα αποτελέσματα για ορισμένες πόλεις.&#34;&#34;&#34;

tsp_dot_attention_model20_path = &#34;tsp_20_dot_attention.pt&#34;
# tsp_dot_attention_model_gaussian_data20_path= &#34;tsp_20_gaussian_data_dot.pt&#34;
# tsp_bahdanau_attention_model_gaussian_data50_path = &#34;tsp_50_bahdanau_attention.pt&#34;
# tsp_dot_attention_convEmbedding_model20_path= &#34;tsp_20_dot_convEmbedding.pt&#34;


tsp_dot_attention_model20_trained = CombinatorialRL(
    embedding_size,
    hidden_size,
    20,
    n_glimpses,
    tanh_exploration,
    use_tanh,
    reward, embedding=&#34;Conv&#34;,
    attention=&#34;Dot&#34;)
tsp_dot_attention_model20_trained.load_state_dict(torch.load(tsp_dot_attention_model20_path))
tsp_dot_attention_model20_trained.eval()

if __name__ == &#39;__main__&#39;:
    # We compare with an untrained model
    tsp_20_model_untrained = CombinatorialRL(
        embedding_size,
        hidden_size,
        20,
        n_glimpses,
        tanh_exploration,
        use_tanh,
        reward, embedding=&#34;Conv&#34;,
        attention=&#34;Dot&#34;)
    tsp_20_model_untrained.eval()

    for batch_id, sample_batch in enumerate(test_loader_5nodes):
        inputs = Variable(sample_batch)
        R_1, probs_1, actions_1, actions_idxs_trained1 = tsp_dot_attention_model20_trained(inputs)

        R, probs, actions, actions_idxs = tsp_20_model_untrained(inputs)

        render(inputs, actions_1, R_1, [0, 1], name_suffix=&#39;Model with dot attention, 20 cities&#39;)
        render(inputs, actions, R, [0, 1], name_suffix=&#39;Untrained model&#39;)

    for batch_id, sample_batch in enumerate(test_loader_5nodes):
        inputs = Variable(sample_batch)
        print(f&#39;model wants {inputs}&#39;)
        R_1, probs_1, actions_1, actions_idxs_trained1 = tsp_dot_attention_model20_trained(inputs)

        R, probs, actions, actions_idxs = tsp_20_model_untrained(inputs)

        render(inputs, actions_1, R_1, [0, 1], name_suffix=&#39;Model with dot attention, 20 cities&#39;)
        render(inputs, actions, R, [0, 1], name_suffix=&#39;Untrained model&#39;)

import folium


def visualize_cvrp_solution(city_coords, tour_indices):
    &#34;&#34;&#34;
    Visualize CVRP solution using folium.

    Parameters:
    - city_coords: List of (lat, lon) tuples representing city coordinates.
    - tour_indices: List of indices representing the order in which cities are visited.

    Returns:
    - m: folium map object.
    &#34;&#34;&#34;

    # Create a folium map centered around the first city
    m = folium.Map(location=city_coords[tour_indices[0]], zoom_start=13)

    # Add points for each city to the map
    for coord in city_coords:
        folium.Marker(coord).add_to(m)

    # Draw lines between cities to represent the tour
    for i in range(1, len(tour_indices)):
        start = city_coords[tour_indices[i - 1]]
        end = city_coords[tour_indices[i]]
        folium.PolyLine([start, end], color=&#34;blue&#34;).add_to(m)

    return m</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="routy.modelsForComparison.tsp.max_grad_norm"><code class="name">var <span class="ident">max_grad_norm</span></code></dt>
<dd>
<div class="desc"><p>Δημιουργούμε τα μοντέλα που θα εκπαιδεύσουμε.</p></div>
</dd>
<dt id="routy.modelsForComparison.tsp.test_loader_5nodes"><code class="name">var <span class="ident">test_loader_5nodes</span></code></dt>
<dd>
<div class="desc"><p>Στη συνέχεια φορτώνουμε τα μοντέλα που εκπαιδεύσαμε και βλέπουμε τα αποτελέσματα για ορισμένες πόλεις.</p></div>
</dd>
<dt id="routy.modelsForComparison.tsp.tsp_20_model_conv_embedding"><code class="name">var <span class="ident">tsp_20_model_conv_embedding</span></code></dt>
<dd>
<div class="desc"><h3>Κλάση για εκπαίδευση μοντέλου</h3>
<p>Στη συνέχεια ορίζουμε μια κλάση στην οποία εκπαιδεύεται το μοντέλο.</p></div>
</dd>
<dt id="routy.modelsForComparison.tsp.val_100_dataset"><code class="name">var <span class="ident">val_100_dataset</span></code></dt>
<dd>
<div class="desc"><h3>Ανταμοιβή</h3>
<p>
Δίνοντας ως είσοδο τις συντεταγμένες των πόλεων, έστω ν ο αριθμός των πόλεων, ο στόχος είναι η εύρεση της σειράς π των πόλεων που πρέπει να επισκεφτούμε ώστε το συνολικό μήκος της διαδρομής που διανύσαμε να είναι ελάχιστο. Ορίζουμε την συνάρτηση που υπολογίζει το μήκος των διαδρομών.
</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="routy.modelsForComparison.tsp.reward"><code class="name flex">
<span>def <span class="ident">reward</span></span>(<span>sample_solution)</span>
</code></dt>
<dd>
<div class="desc"><p>sample_solution seq_len of [batch_size]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reward(sample_solution):
    &#34;&#34;&#34;
    sample_solution seq_len of [batch_size]
    &#34;&#34;&#34;
    batch_size = sample_solution[0].size(0)
    n = len(sample_solution)
    tour_len = Variable(torch.zeros([batch_size]))

    for i in range(n - 1):
        tour_len += torch.norm(sample_solution[i] - sample_solution[i + 1], dim=1)

    tour_len += torch.norm(sample_solution[n - 1] - sample_solution[0], dim=1)

    return tour_len</code></pre>
</details>
</dd>
<dt id="routy.modelsForComparison.tsp.visualize_cvrp_solution"><code class="name flex">
<span>def <span class="ident">visualize_cvrp_solution</span></span>(<span>city_coords, tour_indices)</span>
</code></dt>
<dd>
<div class="desc"><p>Visualize CVRP solution using folium.</p>
<p>Parameters:
- city_coords: List of (lat, lon) tuples representing city coordinates.
- tour_indices: List of indices representing the order in which cities are visited.</p>
<p>Returns:
- m: folium map object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize_cvrp_solution(city_coords, tour_indices):
    &#34;&#34;&#34;
    Visualize CVRP solution using folium.

    Parameters:
    - city_coords: List of (lat, lon) tuples representing city coordinates.
    - tour_indices: List of indices representing the order in which cities are visited.

    Returns:
    - m: folium map object.
    &#34;&#34;&#34;

    # Create a folium map centered around the first city
    m = folium.Map(location=city_coords[tour_indices[0]], zoom_start=13)

    # Add points for each city to the map
    for coord in city_coords:
        folium.Marker(coord).add_to(m)

    # Draw lines between cities to represent the tour
    for i in range(1, len(tour_indices)):
        start = city_coords[tour_indices[i - 1]]
        end = city_coords[tour_indices[i]]
        folium.PolyLine([start, end], color=&#34;blue&#34;).add_to(m)

    return m</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="routy.modelsForComparison.tsp.Attention"><code class="flex name class">
<span>class <span class="ident">Attention</span></span>
<span>(</span><span>hidden_size, use_tanh=False, C=10, name='Bahdanau')</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Attention(nn.Module):
    def __init__(self, hidden_size, use_tanh=False, C=10, name=&#39;Bahdanau&#39;):
        super(Attention, self).__init__()

        self.use_tanh = use_tanh
        self.C = C
        self.name = name

        if name == &#39;Bahdanau&#39;:
            self.W_query = nn.Linear(hidden_size, hidden_size)
            self.W_ref = nn.Conv1d(hidden_size, hidden_size, 1, 1)

            V = torch.FloatTensor(hidden_size)
            self.V = nn.Parameter(V)
            self.V.data.uniform_(-(1. / math.sqrt(hidden_size)), 1. / math.sqrt(hidden_size))

    def forward(self, query, ref):
        &#34;&#34;&#34;
        Args:
            query: [batch_size x hidden_size]
            ref:   ]batch_size x seq_len x hidden_size]
        &#34;&#34;&#34;

        batch_size = ref.size(0)
        seq_len = ref.size(1)

        if self.name == &#39;Bahdanau&#39;:
            ref = ref.permute(0, 2, 1)
            query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]
            ref = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]
            expanded_query = query.repeat(1, 1, seq_len)  # [batch_size x hidden_size x seq_len]
            V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size x 1 x hidden_size]
            logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)

        elif self.name == &#39;Dot&#39;:
            query = query.unsqueeze(2)
            logits = torch.bmm(ref, query).squeeze(2)  # [batch_size x seq_len x 1]
            ref = ref.permute(0, 2, 1)

        else:
            raise NotImplementedError

        if self.use_tanh:
            logits = self.C * F.tanh(logits)
        else:
            logits = logits
        return ref, logits</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.Attention.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.Attention.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.Attention.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.Attention.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, query, ref) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>query</code></strong></dt>
<dd>[batch_size x hidden_size]</dd>
<dt><strong><code>ref</code></strong></dt>
<dd>]batch_size x seq_len x hidden_size]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, query, ref):
    &#34;&#34;&#34;
    Args:
        query: [batch_size x hidden_size]
        ref:   ]batch_size x seq_len x hidden_size]
    &#34;&#34;&#34;

    batch_size = ref.size(0)
    seq_len = ref.size(1)

    if self.name == &#39;Bahdanau&#39;:
        ref = ref.permute(0, 2, 1)
        query = self.W_query(query).unsqueeze(2)  # [batch_size x hidden_size x 1]
        ref = self.W_ref(ref)  # [batch_size x hidden_size x seq_len]
        expanded_query = query.repeat(1, 1, seq_len)  # [batch_size x hidden_size x seq_len]
        V = self.V.unsqueeze(0).unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size x 1 x hidden_size]
        logits = torch.bmm(V, F.tanh(expanded_query + ref)).squeeze(1)

    elif self.name == &#39;Dot&#39;:
        query = query.unsqueeze(2)
        logits = torch.bmm(ref, query).squeeze(2)  # [batch_size x seq_len x 1]
        ref = ref.permute(0, 2, 1)

    else:
        raise NotImplementedError

    if self.use_tanh:
        logits = self.C * F.tanh(logits)
    else:
        logits = logits
    return ref, logits</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="routy.modelsForComparison.tsp.CombinatorialRL"><code class="flex name class">
<span>class <span class="ident">CombinatorialRL</span></span>
<span>(</span><span>embedding_size, hidden_size, seq_len, n_glimpses, tanh_exploration, use_tanh, reward, attention, embedding)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CombinatorialRL(nn.Module):
    def __init__(self,
                 embedding_size,
                 hidden_size,
                 seq_len,
                 n_glimpses,
                 tanh_exploration,
                 use_tanh,
                 reward,
                 attention,
                 embedding):
        super(CombinatorialRL, self).__init__()
        self.reward = reward

        self.actor = PointerNet(
            embedding_size,
            hidden_size,
            seq_len,
            n_glimpses,
            tanh_exploration,
            use_tanh,
            attention,
            embedding)

    def forward(self, inputs):
        &#34;&#34;&#34;
        Args:
            inputs: [batch_size, input_size, seq_len]
        &#34;&#34;&#34;
        batch_size = inputs.size(0)
        input_size = inputs.size(1)
        seq_len = inputs.size(2)

        probs, action_idxs = self.actor(inputs)

        actions = []
        inputs = inputs.transpose(1, 2)
        for action_id in action_idxs:
            actions.append(inputs[[x for x in range(batch_size)], action_id.data, :])

        action_probs = []
        for prob, action_id in zip(probs, action_idxs):
            action_probs.append(prob[[x for x in range(batch_size)], action_id.data])

        R = self.reward(actions)

        return R, action_probs, actions, action_idxs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.CombinatorialRL.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.CombinatorialRL.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.CombinatorialRL.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.CombinatorialRL.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, inputs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>[batch_size, input_size, seq_len]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, inputs):
    &#34;&#34;&#34;
    Args:
        inputs: [batch_size, input_size, seq_len]
    &#34;&#34;&#34;
    batch_size = inputs.size(0)
    input_size = inputs.size(1)
    seq_len = inputs.size(2)

    probs, action_idxs = self.actor(inputs)

    actions = []
    inputs = inputs.transpose(1, 2)
    for action_id in action_idxs:
        actions.append(inputs[[x for x in range(batch_size)], action_id.data, :])

    action_probs = []
    for prob, action_id in zip(probs, action_idxs):
        action_probs.append(prob[[x for x in range(batch_size)], action_id.data])

    R = self.reward(actions)

    return R, action_probs, actions, action_idxs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding"><code class="flex name class">
<span>class <span class="ident">ConvolutionalGraphEmbedding</span></span>
<span>(</span><span>input_size, embedding_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ConvolutionalGraphEmbedding(nn.Module):
    def __init__(self, input_size, embedding_size):
        super(ConvolutionalGraphEmbedding, self).__init__()
        self.embedding_size = embedding_size
        self.embedding = nn.Conv1d(input_size, embedding_size, kernel_size=1)

    def forward(self, inputs):
        batch_size = inputs.size(0)
        embedded = []
        inputs = inputs.unsqueeze(1)
        for i in range(batch_size):
            el = self.embedding(inputs[i, 0])
            embedded.append(torch.unsqueeze(el, 0))
        embedded = torch.cat(embedded, 0)
        return embedded.transpose(2, 1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, inputs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, inputs):
    batch_size = inputs.size(0)
    embedded = []
    inputs = inputs.unsqueeze(1)
    for i in range(batch_size):
        el = self.embedding(inputs[i, 0])
        embedded.append(torch.unsqueeze(el, 0))
    embedded = torch.cat(embedded, 0)
    return embedded.transpose(2, 1)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="routy.modelsForComparison.tsp.GraphEmbedding"><code class="flex name class">
<span>class <span class="ident">GraphEmbedding</span></span>
<span>(</span><span>input_size, embedding_size)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class GraphEmbedding(nn.Module):
    def __init__(self, input_size, embedding_size):
        super(GraphEmbedding, self).__init__()
        self.embedding_size = embedding_size

        self.embedding = nn.Parameter(torch.FloatTensor(input_size, embedding_size))
        self.embedding.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))

    def forward(self, inputs):
        batch_size = inputs.size(0)
        seq_len = inputs.size(2)
        embedding = self.embedding.repeat(batch_size, 1, 1)
        embedded = []
        inputs = inputs.unsqueeze(1)
        for i in range(seq_len):
            embedded.append(torch.bmm(inputs[:, :, :, i].float(), embedding))
        embedded = torch.cat(embedded, 1)
        return embedded</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.GraphEmbedding.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.GraphEmbedding.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.GraphEmbedding.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.GraphEmbedding.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, inputs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, inputs):
    batch_size = inputs.size(0)
    seq_len = inputs.size(2)
    embedding = self.embedding.repeat(batch_size, 1, 1)
    embedded = []
    inputs = inputs.unsqueeze(1)
    for i in range(seq_len):
        embedded.append(torch.bmm(inputs[:, :, :, i].float(), embedding))
    embedded = torch.cat(embedded, 1)
    return embedded</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="routy.modelsForComparison.tsp.PointerNet"><code class="flex name class">
<span>class <span class="ident">PointerNet</span></span>
<span>(</span><span>embedding_size, hidden_size, seq_len, n_glimpses, tanh_exploration, use_tanh, attention, embedding='Graph')</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PointerNet(nn.Module):
    def __init__(self,
                 embedding_size,
                 hidden_size,
                 seq_len,
                 n_glimpses,
                 tanh_exploration,
                 use_tanh,
                 attention,
                 embedding=&#34;Graph&#34;):
        super(PointerNet, self).__init__()

        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.n_glimpses = n_glimpses
        self.seq_len = seq_len

        if embedding == &#34;Graph&#34;:
            self.embedding = GraphEmbedding(2, embedding_size)
        elif embedding == &#34;Conv&#34;:
            self.embedding = ConvolutionalGraphEmbedding(2, embedding_size)

        self.encoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)
        self.decoder = nn.LSTM(embedding_size, hidden_size, batch_first=True)
        self.pointer = Attention(hidden_size, use_tanh=use_tanh, C=tanh_exploration, name=attention)
        self.glimpse = Attention(hidden_size, use_tanh=False, name=attention)

        self.decoder_start_input = nn.Parameter(torch.FloatTensor(embedding_size))
        self.decoder_start_input.data.uniform_(-(1. / math.sqrt(embedding_size)), 1. / math.sqrt(embedding_size))

    def apply_mask_to_logits(self, logits, mask, idxs):
        batch_size = logits.size(0)
        clone_mask = mask.clone()

        if idxs is not None:
            clone_mask[[i for i in range(batch_size)], idxs.data] = 1
            logits[clone_mask] = -np.inf

        return logits, clone_mask

    def forward(self, inputs):
        &#34;&#34;&#34;
        Args:
            inputs: [batch_size x 1 x sourceL]
        &#34;&#34;&#34;
        batch_size = inputs.size(0)
        seq_len = inputs.size(2)

        embedded = self.embedding(inputs)
        encoder_outputs, (hidden, context) = self.encoder(embedded)

        prev_probs = []
        prev_idxs = []
        mask = torch.zeros(batch_size, seq_len).byte()

        idxs = None

        decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)

        for i in range(seq_len):

            _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))

            query = hidden.squeeze(0)
            for i in range(self.n_glimpses):
                ref, logits = self.glimpse(query, encoder_outputs)
                logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
                query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)

            _, logits = self.pointer(query, encoder_outputs)
            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
            probs = F.softmax(logits)

            idxs = probs.multinomial(1).squeeze(1)

            for old_idxs in prev_idxs:
                if old_idxs.eq(idxs).data.any():
                    print(seq_len)
                    print(&#39; RESAMPLE!&#39;)
                    idxs = probs.multinomial(1).squeeze(1)
                    break

            decoder_input = embedded[[i for i in range(batch_size)], idxs.data, :]

            prev_probs.append(probs)
            prev_idxs.append(idxs)

        return prev_probs, prev_idxs</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.PointerNet.call_super_init"><code class="name">var <span class="ident">call_super_init</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.PointerNet.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="routy.modelsForComparison.tsp.PointerNet.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.PointerNet.apply_mask_to_logits"><code class="name flex">
<span>def <span class="ident">apply_mask_to_logits</span></span>(<span>self, logits, mask, idxs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def apply_mask_to_logits(self, logits, mask, idxs):
    batch_size = logits.size(0)
    clone_mask = mask.clone()

    if idxs is not None:
        clone_mask[[i for i in range(batch_size)], idxs.data] = 1
        logits[clone_mask] = -np.inf

    return logits, clone_mask</code></pre>
</details>
</dd>
<dt id="routy.modelsForComparison.tsp.PointerNet.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, inputs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>inputs</code></strong></dt>
<dd>[batch_size x 1 x sourceL]</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, inputs):
    &#34;&#34;&#34;
    Args:
        inputs: [batch_size x 1 x sourceL]
    &#34;&#34;&#34;
    batch_size = inputs.size(0)
    seq_len = inputs.size(2)

    embedded = self.embedding(inputs)
    encoder_outputs, (hidden, context) = self.encoder(embedded)

    prev_probs = []
    prev_idxs = []
    mask = torch.zeros(batch_size, seq_len).byte()

    idxs = None

    decoder_input = self.decoder_start_input.unsqueeze(0).repeat(batch_size, 1)

    for i in range(seq_len):

        _, (hidden, context) = self.decoder(decoder_input.unsqueeze(1), (hidden, context))

        query = hidden.squeeze(0)
        for i in range(self.n_glimpses):
            ref, logits = self.glimpse(query, encoder_outputs)
            logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
            query = torch.bmm(ref, F.softmax(logits).unsqueeze(2)).squeeze(2)

        _, logits = self.pointer(query, encoder_outputs)
        logits, mask = self.apply_mask_to_logits(logits, mask, idxs)
        probs = F.softmax(logits)

        idxs = probs.multinomial(1).squeeze(1)

        for old_idxs in prev_idxs:
            if old_idxs.eq(idxs).data.any():
                print(seq_len)
                print(&#39; RESAMPLE!&#39;)
                idxs = probs.multinomial(1).squeeze(1)
                break

        decoder_input = embedded[[i for i in range(batch_size)], idxs.data, :]

        prev_probs.append(probs)
        prev_idxs.append(idxs)

    return prev_probs, prev_idxs</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="routy.modelsForComparison.tsp.TSPDataset"><code class="flex name class">
<span>class <span class="ident">TSPDataset</span></span>
<span>(</span><span>num_nodes, num_samples, random_seed=111)</span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
sampler that yields integral indices.
To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TSPDataset(Dataset):
    def __init__(self, num_nodes, num_samples, random_seed=111):
        super(TSPDataset, self).__init__()
        torch.manual_seed(random_seed)

        self.data_set = []
        for l in tqdm(range(num_samples)):
            x = torch.FloatTensor(2, num_nodes).uniform_(0, 1)
            self.data_set.append(x)

        self.size = len(self.data_set)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data_set[idx]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
</dd>
<dt id="routy.modelsForComparison.tsp.TSPDatasetGaussian"><code class="flex name class">
<span>class <span class="ident">TSPDatasetGaussian</span></span>
<span>(</span><span>num_nodes, num_samples, random_seed=111)</span>
</code></dt>
<dd>
<div class="desc"><p>An abstract class representing a :class:<code>Dataset</code>.</p>
<p>All datasets that represent a map from keys to data samples should subclass
it. All subclasses should overwrite :meth:<code>__getitem__</code>, supporting fetching a
data sample for a given key. Subclasses could also optionally overwrite
:meth:<code>__len__</code>, which is expected to return the size of the dataset by many
:class:<code>~torch.utils.data.Sampler</code> implementations and the default options
of :class:<code>~torch.utils.data.DataLoader</code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>:class:<code>~torch.utils.data.DataLoader</code> by default constructs a index
sampler that yields integral indices.
To make it work with a map-style
dataset with non-integral indices/keys, a custom sampler must be provided.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TSPDatasetGaussian(Dataset):
    def __init__(self, num_nodes, num_samples, random_seed=111):
        super(TSPDatasetGaussian, self).__init__()
        torch.manual_seed(random_seed)

        self.data_set = []
        for l in tqdm(range(num_samples)):
            m = torch.distributions.normal.Normal(torch.tensor(0.0), torch.tensor(1.0))
            x = m.sample(sample_shape=(2, num_nodes))
            self.data_set.append(x)

        self.size = len(self.data_set)

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        return self.data_set[idx]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.utils.data.dataset.Dataset</li>
<li>typing.Generic</li>
</ul>
</dd>
<dt id="routy.modelsForComparison.tsp.TrainModel"><code class="flex name class">
<span>class <span class="ident">TrainModel</span></span>
<span>(</span><span>model, train_dataset, val_dataset, batch_size=128, threshold=None, max_grad_norm=2.0, modelName='model.pt')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TrainModel:
    def __init__(self, model, train_dataset, val_dataset, batch_size=128, threshold=None, max_grad_norm=2.,
                 modelName=&#34;model.pt&#34;):
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.batch_size = batch_size
        self.threshold = threshold
        self.modelName = modelName
        self.train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
        self.val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)

        self.actor_optim = optim.Adam(model.actor.parameters(), lr=1e-4)
        self.max_grad_norm = max_grad_norm

        self.train_tour = []
        self.val_tour = []
        self.advantage = []
        self.train_loss = []
        self.distances = []
        self.epochs = 0

    def train_and_validate(self, n_epochs):
        critic_exp_mvg_avg = torch.zeros(1)
        start_time = time.time()

        for epoch in range(n_epochs):
            loss_at_epoch = 0.0
            distance = 0.0
            val_distance = 0.0
            advantage_at_epoch = 0.0

            for batch_id, sample_batch in enumerate(self.train_loader):
                self.model.train()

                inputs = Variable(sample_batch)

                R, probs, actions, actions_idxs = self.model(inputs)

                if batch_id == 0:
                    critic_exp_mvg_avg = R.mean()
                else:
                    critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())

                advantage = R - critic_exp_mvg_avg
                advantage_at_epoch += advantage.detach().mean().item()

                logprobs = 0
                for prob in probs:
                    logprob = torch.log(prob)
                    logprobs += logprob
                logprobs[logprobs &lt; -1000] = 0.

                reinforce = advantage * logprobs
                actor_loss = reinforce.mean()

                self.actor_optim.zero_grad()
                actor_loss.backward()
                torch.nn.utils.clip_grad_norm(self.model.actor.parameters(),
                                              float(self.max_grad_norm), norm_type=2)

                self.actor_optim.step()

                critic_exp_mvg_avg = critic_exp_mvg_avg.detach()

                loss_at_epoch += actor_loss.detach().item()
                distance += R.mean().data.item()

                if batch_id % 100 == 0:
                    self.model.eval()
                    for val_batch in self.val_loader:
                        inputs = Variable(val_batch)

                        R, probs, actions, actions_idxs = self.model(inputs)
                        val_distance += R.mean().data.item()

            # epoch finished

            self.val_tour.append(val_distance)
            self.train_loss.append(loss_at_epoch)
            self.train_tour.append(distance)
            self.advantage.append(advantage_at_epoch)
            self.plotTrainAndValidationTourLength(self.epochs)
            self.plotTrainLossAndAdvantage(self.epochs)

            torch.save(self.model.state_dict(), self.modelName)
            if self.threshold and self.train_tour[-1] &lt; self.threshold:
                print(f&#34;Early stopping at epoch {epoch}&#34;)
                break

            self.epochs += 1

        # Training finished
        training_time_in_secs = time.time() - start_time
        print(f&#39;Trainig for {training_time_in_secs // 60} minutes&#39;)

    def plotTrainLossAndAdvantage(self, epoch):
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.ylabel(&#34;Training loss&#34;)
        plt.xlabel(&#34;Epoch&#34;)
        plt.title(&#39;Loss: epoch %s loss %s&#39; % (epoch, self.train_tour[-1] if len(self.train_tour) else &#39;collecting&#39;))
        plt.plot(self.train_loss)
        plt.grid()
        plt.subplot(132)
        plt.title(
            &#39;Advantage: epoch %s reward %s&#39; % (epoch, self.advantage[-1] if len(self.advantage) else &#39;collecting&#39;))
        plt.plot(self.advantage)
        plt.grid()
        plt.ylabel(&#34;Training advantage&#34;)
        plt.xlabel(&#34;Epoch&#34;)
        plt.savefig(f&#34;trainLossAndAdvantage{self.modelName}.png&#34;)

        plt.show()
        plt.clf()

    def plotTrainAndValidationTourLength(self, epoch):
        clear_output(True)
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.ylabel(&#34;Tour length&#34;)
        plt.xlabel(&#34;Epoch&#34;)
        plt.title(&#39;train tour length: epoch %s,tour length %s&#39; % (
            epoch, self.train_tour[-1] if len(self.train_tour) else &#39;collecting&#39;))
        plt.plot(self.train_tour)
        plt.grid()
        plt.subplot(132)
        plt.title(&#39;val tour length: epoch %s,tour length %s&#39; % (
        epoch, self.val_tour[-1] if len(self.val_tour) else &#39;collecting&#39;))
        plt.plot(self.val_tour)
        plt.grid()
        plt.savefig(f&#34;tourLength{self.modelName}.png&#34;)

        plt.show()
        plt.clf()</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="routy.modelsForComparison.tsp.TrainModel.plotTrainAndValidationTourLength"><code class="name flex">
<span>def <span class="ident">plotTrainAndValidationTourLength</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plotTrainAndValidationTourLength(self, epoch):
    clear_output(True)
    plt.figure(figsize=(20, 5))
    plt.subplot(131)
    plt.ylabel(&#34;Tour length&#34;)
    plt.xlabel(&#34;Epoch&#34;)
    plt.title(&#39;train tour length: epoch %s,tour length %s&#39; % (
        epoch, self.train_tour[-1] if len(self.train_tour) else &#39;collecting&#39;))
    plt.plot(self.train_tour)
    plt.grid()
    plt.subplot(132)
    plt.title(&#39;val tour length: epoch %s,tour length %s&#39; % (
    epoch, self.val_tour[-1] if len(self.val_tour) else &#39;collecting&#39;))
    plt.plot(self.val_tour)
    plt.grid()
    plt.savefig(f&#34;tourLength{self.modelName}.png&#34;)

    plt.show()
    plt.clf()</code></pre>
</details>
</dd>
<dt id="routy.modelsForComparison.tsp.TrainModel.plotTrainLossAndAdvantage"><code class="name flex">
<span>def <span class="ident">plotTrainLossAndAdvantage</span></span>(<span>self, epoch)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plotTrainLossAndAdvantage(self, epoch):
    plt.figure(figsize=(20, 5))
    plt.subplot(131)
    plt.ylabel(&#34;Training loss&#34;)
    plt.xlabel(&#34;Epoch&#34;)
    plt.title(&#39;Loss: epoch %s loss %s&#39; % (epoch, self.train_tour[-1] if len(self.train_tour) else &#39;collecting&#39;))
    plt.plot(self.train_loss)
    plt.grid()
    plt.subplot(132)
    plt.title(
        &#39;Advantage: epoch %s reward %s&#39; % (epoch, self.advantage[-1] if len(self.advantage) else &#39;collecting&#39;))
    plt.plot(self.advantage)
    plt.grid()
    plt.ylabel(&#34;Training advantage&#34;)
    plt.xlabel(&#34;Epoch&#34;)
    plt.savefig(f&#34;trainLossAndAdvantage{self.modelName}.png&#34;)

    plt.show()
    plt.clf()</code></pre>
</details>
</dd>
<dt id="routy.modelsForComparison.tsp.TrainModel.train_and_validate"><code class="name flex">
<span>def <span class="ident">train_and_validate</span></span>(<span>self, n_epochs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_and_validate(self, n_epochs):
    critic_exp_mvg_avg = torch.zeros(1)
    start_time = time.time()

    for epoch in range(n_epochs):
        loss_at_epoch = 0.0
        distance = 0.0
        val_distance = 0.0
        advantage_at_epoch = 0.0

        for batch_id, sample_batch in enumerate(self.train_loader):
            self.model.train()

            inputs = Variable(sample_batch)

            R, probs, actions, actions_idxs = self.model(inputs)

            if batch_id == 0:
                critic_exp_mvg_avg = R.mean()
            else:
                critic_exp_mvg_avg = (critic_exp_mvg_avg * beta) + ((1. - beta) * R.mean())

            advantage = R - critic_exp_mvg_avg
            advantage_at_epoch += advantage.detach().mean().item()

            logprobs = 0
            for prob in probs:
                logprob = torch.log(prob)
                logprobs += logprob
            logprobs[logprobs &lt; -1000] = 0.

            reinforce = advantage * logprobs
            actor_loss = reinforce.mean()

            self.actor_optim.zero_grad()
            actor_loss.backward()
            torch.nn.utils.clip_grad_norm(self.model.actor.parameters(),
                                          float(self.max_grad_norm), norm_type=2)

            self.actor_optim.step()

            critic_exp_mvg_avg = critic_exp_mvg_avg.detach()

            loss_at_epoch += actor_loss.detach().item()
            distance += R.mean().data.item()

            if batch_id % 100 == 0:
                self.model.eval()
                for val_batch in self.val_loader:
                    inputs = Variable(val_batch)

                    R, probs, actions, actions_idxs = self.model(inputs)
                    val_distance += R.mean().data.item()

        # epoch finished

        self.val_tour.append(val_distance)
        self.train_loss.append(loss_at_epoch)
        self.train_tour.append(distance)
        self.advantage.append(advantage_at_epoch)
        self.plotTrainAndValidationTourLength(self.epochs)
        self.plotTrainLossAndAdvantage(self.epochs)

        torch.save(self.model.state_dict(), self.modelName)
        if self.threshold and self.train_tour[-1] &lt; self.threshold:
            print(f&#34;Early stopping at epoch {epoch}&#34;)
            break

        self.epochs += 1

    # Training finished
    training_time_in_secs = time.time() - start_time
    print(f&#39;Trainig for {training_time_in_secs // 60} minutes&#39;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="routy.modelsForComparison" href="index.html">routy.modelsForComparison</a></code></li>
</ul>
</li>
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.max_grad_norm" href="#routy.modelsForComparison.tsp.max_grad_norm">max_grad_norm</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.test_loader_5nodes" href="#routy.modelsForComparison.tsp.test_loader_5nodes">test_loader_5nodes</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.tsp_20_model_conv_embedding" href="#routy.modelsForComparison.tsp.tsp_20_model_conv_embedding">tsp_20_model_conv_embedding</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.val_100_dataset" href="#routy.modelsForComparison.tsp.val_100_dataset">val_100_dataset</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.reward" href="#routy.modelsForComparison.tsp.reward">reward</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.visualize_cvrp_solution" href="#routy.modelsForComparison.tsp.visualize_cvrp_solution">visualize_cvrp_solution</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.Attention" href="#routy.modelsForComparison.tsp.Attention">Attention</a></code></h4>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.Attention.call_super_init" href="#routy.modelsForComparison.tsp.Attention.call_super_init">call_super_init</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.Attention.dump_patches" href="#routy.modelsForComparison.tsp.Attention.dump_patches">dump_patches</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.Attention.forward" href="#routy.modelsForComparison.tsp.Attention.forward">forward</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.Attention.training" href="#routy.modelsForComparison.tsp.Attention.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.CombinatorialRL" href="#routy.modelsForComparison.tsp.CombinatorialRL">CombinatorialRL</a></code></h4>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.CombinatorialRL.call_super_init" href="#routy.modelsForComparison.tsp.CombinatorialRL.call_super_init">call_super_init</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.CombinatorialRL.dump_patches" href="#routy.modelsForComparison.tsp.CombinatorialRL.dump_patches">dump_patches</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.CombinatorialRL.forward" href="#routy.modelsForComparison.tsp.CombinatorialRL.forward">forward</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.CombinatorialRL.training" href="#routy.modelsForComparison.tsp.CombinatorialRL.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding" href="#routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding">ConvolutionalGraphEmbedding</a></code></h4>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.call_super_init" href="#routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.call_super_init">call_super_init</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.dump_patches" href="#routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.dump_patches">dump_patches</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.forward" href="#routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.forward">forward</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.training" href="#routy.modelsForComparison.tsp.ConvolutionalGraphEmbedding.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.GraphEmbedding" href="#routy.modelsForComparison.tsp.GraphEmbedding">GraphEmbedding</a></code></h4>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.GraphEmbedding.call_super_init" href="#routy.modelsForComparison.tsp.GraphEmbedding.call_super_init">call_super_init</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.GraphEmbedding.dump_patches" href="#routy.modelsForComparison.tsp.GraphEmbedding.dump_patches">dump_patches</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.GraphEmbedding.forward" href="#routy.modelsForComparison.tsp.GraphEmbedding.forward">forward</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.GraphEmbedding.training" href="#routy.modelsForComparison.tsp.GraphEmbedding.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.PointerNet" href="#routy.modelsForComparison.tsp.PointerNet">PointerNet</a></code></h4>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.PointerNet.apply_mask_to_logits" href="#routy.modelsForComparison.tsp.PointerNet.apply_mask_to_logits">apply_mask_to_logits</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.PointerNet.call_super_init" href="#routy.modelsForComparison.tsp.PointerNet.call_super_init">call_super_init</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.PointerNet.dump_patches" href="#routy.modelsForComparison.tsp.PointerNet.dump_patches">dump_patches</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.PointerNet.forward" href="#routy.modelsForComparison.tsp.PointerNet.forward">forward</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.PointerNet.training" href="#routy.modelsForComparison.tsp.PointerNet.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.TSPDataset" href="#routy.modelsForComparison.tsp.TSPDataset">TSPDataset</a></code></h4>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.TSPDatasetGaussian" href="#routy.modelsForComparison.tsp.TSPDatasetGaussian">TSPDatasetGaussian</a></code></h4>
</li>
<li>
<h4><code><a title="routy.modelsForComparison.tsp.TrainModel" href="#routy.modelsForComparison.tsp.TrainModel">TrainModel</a></code></h4>
<ul class="">
<li><code><a title="routy.modelsForComparison.tsp.TrainModel.plotTrainAndValidationTourLength" href="#routy.modelsForComparison.tsp.TrainModel.plotTrainAndValidationTourLength">plotTrainAndValidationTourLength</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.TrainModel.plotTrainLossAndAdvantage" href="#routy.modelsForComparison.tsp.TrainModel.plotTrainLossAndAdvantage">plotTrainLossAndAdvantage</a></code></li>
<li><code><a title="routy.modelsForComparison.tsp.TrainModel.train_and_validate" href="#routy.modelsForComparison.tsp.TrainModel.train_and_validate">train_and_validate</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>